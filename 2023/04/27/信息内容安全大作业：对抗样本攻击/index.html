<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>信å信息内容安全ç¬大作业：对抗样本攻击 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="信息与内容安全第一次实验实验二虚假人脸检测实验实验摘要给定一个人脸数据集，其中包含1000张真实人脸，1000张虚假人脸。将其中500张真实人脸和500张虚假人脸作为训练集，其余作为测试集。根据给定数据集训练训练一个虚假人脸检测器，该检测器本质就是一个二分类分类器。要求利用Pytorch框架任意设计一种神经网络模型进行分类。 实验原理攻击方法添加不可察觉对图像的扰动将会导致截然不同的模型性能。本次">
<meta property="og:type" content="article">
<meta property="og:title" content="信å信息内容安全ç¬大作业：对抗样本攻击">
<meta property="og:url" content="http://example.com/2023/04/27/%E4%BF%A1%E6%81%AF%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E5%A4%A7%E4%BD%9C%E4%B8%9A%EF%BC%9A%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="信息与内容安全第一次实验实验二虚假人脸检测实验实验摘要给定一个人脸数据集，其中包含1000张真实人脸，1000张虚假人脸。将其中500张真实人脸和500张虚假人脸作为训练集，其余作为测试集。根据给定数据集训练训练一个虚假人脸检测器，该检测器本质就是一个二分类分类器。要求利用Pytorch框架任意设计一种神经网络模型进行分类。 实验原理攻击方法添加不可察觉对图像的扰动将会导致截然不同的模型性能。本次">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/panda.png">
<meta property="og:image" content="http://example.com/%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="http://example.com/out.png">
<meta property="og:image" content="http://example.com/vs.png">
<meta property="og:image" content="http://example.com/shuzi.png">
<meta property="og:image" content="http://example.com/%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="http://example.com/Figure_1.png">
<meta property="article:published_time" content="2023-04-27T13:03:57.000Z">
<meta property="article:modified_time" content="2023-04-27T13:04:57.130Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/panda.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-信息内容安全大作业：对抗样本攻击" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/27/%E4%BF%A1%E6%81%AF%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E5%A4%A7%E4%BD%9C%E4%B8%9A%EF%BC%9A%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/" class="article-date">
  <time class="dt-published" datetime="2023-04-27T13:03:57.000Z" itemprop="datePublished">2023-04-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      信å信息内容安全ç¬大作业：对抗样本攻击
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="信息与内容安全第一次实验"><a href="#信息与内容安全第一次实验" class="headerlink" title="信息与内容安全第一次实验"></a>信息与内容安全第一次实验</h1><h2 id="实验二"><a href="#实验二" class="headerlink" title="实验二"></a>实验二</h2><h3 id="虚假人脸检测实验"><a href="#虚假人脸检测实验" class="headerlink" title="虚假人脸检测实验"></a>虚假人脸检测实验</h3><h3 id="实验摘要"><a href="#实验摘要" class="headerlink" title="实验摘要"></a>实验摘要</h3><p>给定一个人脸数据集，其中包含1000张真实人脸，1000张虚假人脸。将其中500张真实人脸和500张虚假人脸作为训练集，其余作为测试集。<br>根据给定数据集训练训练一个虚假人脸检测器，该检测器本质就是一个二分类分类器。要求利用Pytorch框架任意设计一种神经网络模型进行分类。</p>
<h3 id="实验原理"><a href="#实验原理" class="headerlink" title="实验原理"></a>实验原理</h3><h4 id="攻击方法"><a href="#攻击方法" class="headerlink" title="攻击方法"></a>攻击方法</h4><p>添加不可察觉对图像的扰动将会导致截然不同的模型性能。<br>本次实验使用快速梯度符号攻击（FGSM）来欺骗MNIST分类器。</p>
<p>有许多类别的对抗性攻击，每种攻击都有不同的目标和对攻击者知识的假设。但是，总体而言，首要目标是向输入数据添加最少量的扰动，从而导致所需的错误分类。攻击者的知识有几种假设，其中两种是：白盒和黑匣子。</p>
<p>白盒攻击假设攻击者具有对模型的全部了解和访问权限，包括体系结构、输入、输出和权重；黑盒攻击假定攻击者只能访问模型的输入和输出，而对底层体系结构或权重一无所知。<br>错误分类的目标意味着对手只希望输出分类是错误的，但并不关心新分类是什么；源&#x2F;目标错误分类意味着对手想要更改原始属于特定源类的图像，以便将其分类为特定目标类。<br>在这种情况下，FGSM 攻击是白盒攻击，其目标是错误分类。</p>
<h4 id="FGSM原理简介"><a href="#FGSM原理简介" class="headerlink" title="FGSM原理简介"></a>FGSM原理简介</h4><p>FGSM旨在通过利用神经网络的学习方式梯度来攻击神经网络。 这个想法很简单，而不是通过调整基于反向传播梯度的权重来最小化损失，攻击调整输入数据以基于相同的反向传播梯度最大化损失。 换句话说，攻击使用输入数据的损失梯度，然后调整输入数据以最大化损失。</p>
<p><img src="/panda.png" alt="p1"></p>
<p>从图中，当输入原始图像$\mathbf{x}$时，它被正确分类为“熊猫”，其中，$y$ 是$\mathbf{x}$的真是标签，$\mathbf{\theta}$表示模型参数。$J(\mathbf{\theta}, \mathbf{x}, y)$ 是用于训练网络的损失函数。攻击方将梯度反向传播回输入的数据来进行计算$\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$。在最大化损失的方向上(即$sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y))$)，它通过一个小的<em>step</em>（$\epsilon$ 或 图片中的$0.007$））来调整输入数据。 由此产生的扰动图像 $x’$ 。然后 $x’$被目标网络<em>错误分类</em>成“长臂猿”，而其仍然显示为“熊猫”。</p>
<h3 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h3><h4 id="实验思路"><a href="#实验思路" class="headerlink" title="实验思路"></a>实验思路</h4><h4 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h4><ol>
<li><strong>输入</strong><br>本实验共三个参数：</li>
</ol>
<p><code>epsilons - 取值范围为[0,1]</code> - 要用于运行的epsilon值列表。其中0在列表中代表模型在原始测试集上的性能。epsilon越大，扰动越明显，在降低模型准确性方面更有效。由于数据范围在[0,1]之间，因此epsilon值不应超过1。</p>
<p><code>pretrained_model</code> - 预训练的MNIST模型的路径，该模型是使用pytorch&#x2F;examples&#x2F;mnist进行训练的。预训练模型在官网进行<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h">下载</a>。</p>
<p><code>use_cuda</code> - 使用具有cuda的GPU进行训练时，取值为<code>True</code>，不需要时，取值为<code>False</code>。本实验使用CPU进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epsilons = [<span class="number">0</span>, <span class="number">.05</span>, <span class="number">.1</span>, <span class="number">.15</span>, <span class="number">.2</span>, <span class="number">.25</span>, <span class="number">.3</span>]</span><br><span class="line">pretrained_model = <span class="string">&quot;data/lenet_mnist_model.pth&quot;</span></span><br><span class="line">use_cuda=<span class="literal">True</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>建立被攻击模型</strong></li>
</ol>
<p>受攻击的模型是来自于<code>pytorch/examples/mnist</code>的MNIST模型。定义模型和数据加载器，初始化模型并加载预训练权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LeNet Model definition</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST Test dataset and dataloader declaration</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            ])),</span><br><span class="line">        batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define what device we are using</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CUDA Available: &quot;</span>,torch.cuda.is_available())</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> (use_cuda <span class="keyword">and</span> torch.cuda.is_available()) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the network</span></span><br><span class="line">model = Net().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.load_state_dict(torch.load(pretrained_model, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the model in evaluation mode. In this case this is for the Dropout layers</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>FGSM攻击</strong><br>定义通过扰动原始输入来创建对抗样本的函数<code>fgsm_attack</code> 。<br>函数有三个输入：</li>
</ol>
<ul>
<li><em>image</em>：原始的干净图像（$x$）</li>
<li><em>epsilon</em>：像素级别的扰动量（$\epsilon$）</li>
<li><em>data_grad</em>：损失 w.r.t 输入图像的梯度($\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)$)。</li>
</ul>
<p>该函数通过下列公式来创造扰动图像：<br><img src="/%E5%85%AC%E5%BC%8F.png" alt="公式"></p>
<p>最后，为了保持数据的原始范围，扰动图像被裁剪到 $[0,1]$ 范围内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FGSM attack code</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fgsm_attack</span>(<span class="params">image, epsilon, data_grad</span>):</span><br><span class="line">    <span class="comment"># Collect the element-wise sign of the data gradient</span></span><br><span class="line">    sign_data_grad = data_grad.sign()</span><br><span class="line">    <span class="comment"># Create the perturbed image by adjusting each pixel of the input image</span></span><br><span class="line">    perturbed_image = image + epsilon*sign_data_grad</span><br><span class="line">    <span class="comment"># Adding clipping to maintain [0,1] range</span></span><br><span class="line">    perturbed_image = torch.clamp(perturbed_image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Return the perturbed image</span></span><br><span class="line">    <span class="keyword">return</span> perturbed_image</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><strong>测试函数</strong></li>
</ol>
<p>定义<code>test</code>函数：<br>每次调用此测试函数都会执行完整的测试步MNIST 测试集并报告最终准确度。 但是，请注意<br>此函数还需要一个 <em>epsilon</em> 输入。 这是因为<code>test</code> 函数报告受到攻击的模型的准确性来自具有实力 $\epsilon$ 的对手。 更具体地说，对于测试集中的每个样本，函数计算的梯度损失 w.r.t 输入数据（$data_grad$），创建扰动<br>带有 <code>fgsm_attack</code>($perturbed_data$) 的图像，然后检查以查看如果扰动的例子是对抗性的。 除了测试模型的准确性，该函数还保存并返回一些成功的对抗性例子将在稍后可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params"> model, device, test_loader, epsilon </span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Accuracy counter</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    adv_examples = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop over all examples in test set</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the data and label to the device</span></span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set requires_grad attribute of tensor. Important for Attack</span></span><br><span class="line">        data.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass the data through the model</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        init_pred = output.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the initial prediction is wrong, don&#x27;t bother attacking, just move on</span></span><br><span class="line">        <span class="keyword">if</span> init_pred.item() != target.item():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the loss</span></span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero all existing gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate gradients of model in backward pass</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Collect ``datagrad``</span></span><br><span class="line">        data_grad = data.grad.data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Call FGSM Attack</span></span><br><span class="line">        perturbed_data = fgsm_attack(data, epsilon, data_grad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Re-classify the perturbed image</span></span><br><span class="line">        output = model(perturbed_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for success</span></span><br><span class="line">        final_pred = output.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">        <span class="keyword">if</span> final_pred.item() == target.item():</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Special case for saving 0 epsilon examples</span></span><br><span class="line">            <span class="keyword">if</span> (epsilon == <span class="number">0</span>) <span class="keyword">and</span> (<span class="built_in">len</span>(adv_examples) &lt; <span class="number">5</span>):</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Save some adv examples for visualization later</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(adv_examples) &lt; <span class="number">5</span>:</span><br><span class="line">                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()</span><br><span class="line">                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate final accuracy for this epsilon</span></span><br><span class="line">    final_acc = correct/<span class="built_in">float</span>(<span class="built_in">len</span>(test_loader))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epsilon, correct, <span class="built_in">len</span>(test_loader), final_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the accuracy and an adversarial example</span></span><br><span class="line">    <span class="keyword">return</span> final_acc, adv_examples</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><strong>运行攻击</strong></li>
</ol>
<p>实现的最后一部分是实际运行攻击。在这里，我们针对<em>epsilons</em>输入中的每个<em>epsilon</em>值运行一个完整的测试步骤。对于每个<em>epsilon</em>值，我们还保存最终的准确性和一些成功的对抗性示例，以在接下来的部分中绘制。请注意，随着<em>epsilon</em>值的增加，打印的准确性会降低。此外，请注意，<em>ϵ&#x3D;0</em>的情况表示原始测试准确性，没有受到攻击。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">accuracies = []</span><br><span class="line">examples = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run test for each epsilon</span></span><br><span class="line"><span class="keyword">for</span> eps <span class="keyword">in</span> epsilons:</span><br><span class="line">    acc, ex = test(model, device, test_loader, eps)</span><br><span class="line">    accuracies.append(acc)</span><br><span class="line">    examples.append(ex)</span><br></pre></td></tr></table></figure>
<p>####实验结果<br><img src="/out.png" alt="out"></p>
<h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><p>第一个结果是准确率与epsilon的曲线图。正如前面所提到的，随着epsilon的增加，我们预计测试准确率会降低。这是因为更大的epsilon意味着我们朝着最大化损失的方向迈出了更大的一步。请注意，尽管epsilon值是线性间隔的，但曲线趋势并不是线性的。例如，在ϵ&#x3D;0.05处的准确率仅比ϵ&#x3D;0时低约4％，但在ϵ&#x3D;0.2处的准确率比ϵ&#x3D;0.15时低25％。此外，请注意，对于10类分类器，模型的准确率在ϵ&#x3D;0.25和ϵ&#x3D;0.3之间达到随机准确率。</p>
<p><img src="/vs.png" alt="acc-ep"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(epsilons, accuracies, <span class="string">&quot;*-&quot;</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">1.1</span>, step=<span class="number">0.1</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">.35</span>, step=<span class="number">0.05</span>))</span><br><span class="line">plt.title(<span class="string">&quot;Accuracy vs Epsilon&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epsilon&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Accuracy&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>对抗样本示例</p>
<p>还记得天下没有免费的午餐吗？ 在这种情况下，随着 epsilon 的增加<br>测试精度降低 <strong>但是</strong> 扰动变得更容易<br>可感知的。 实际上，在准确性之间存在权衡<br>攻击者必须考虑的退化和可感知性。 在这里，我们<br>在每个 epsilon 展示一些成功的对抗性例子<br>价值。 图中的每一行都显示不同的 epsilon 值。 首先<br>行是代表原始的 $\epsilon&#x3D;0$ 示例<br>没有扰动的“干净”图像。 每个图像的标题显示<br>“原始分类 -&gt; 对抗性分类。” 注意，<br>扰动在 $\epsilon&#x3D;0.15$ 时开始变得明显并且是<br>在 $\epsilon&#x3D;0.3$ 时非常明显。 然而，在所有情况下，人类都是<br>尽管增加了噪音，仍然能够识别正确的类别。<br><img src="/shuzi.png" alt="shuzi"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot several examples of adversarial samples at each epsilon</span></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(epsilons)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(examples[i])):</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        plt.subplot(<span class="built_in">len</span>(epsilons),<span class="built_in">len</span>(examples[<span class="number">0</span>]),cnt)</span><br><span class="line">        plt.xticks([], [])</span><br><span class="line">        plt.yticks([], [])</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            plt.ylabel(<span class="string">&quot;Eps: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epsilons[i]), fontsize=<span class="number">14</span>)</span><br><span class="line">        orig,adv,ex = examples[i][j]</span><br><span class="line">        plt.title(<span class="string">&quot;&#123;&#125; -&gt; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(orig, adv))</span><br><span class="line">        plt.imshow(ex, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>###实验小结</p>
<ol>
<li><strong>实验中遇到的问题</strong><br>刚开始做实验的时候，我反复运行多次代码，每次得到的结果都不尽相同，且准确率比官方文档的低很多，其中一次如下图所示：<br><img src="/%E7%BB%93%E6%9E%9C.png" alt="badout"><br><img src="/Figure_1.png" alt="badvs"><br>经过丢代码的分析，</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/04/27/%E4%BF%A1%E6%81%AF%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E5%A4%A7%E4%BD%9C%E4%B8%9A%EF%BC%9A%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/" data-id="clgz51y100002dcck91dpaqfs" data-title="信å信息内容安全ç¬大作业：对抗样本攻击" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/04/27/%E4%BD%A0%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AFycy/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">你好，我是ycy</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/04/27/%E4%BF%A1%E6%81%AF%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E5%A4%A7%E4%BD%9C%E4%B8%9A%EF%BC%9A%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/">信å信息内容安全ç¬大作业：对抗样本攻击</a>
          </li>
        
          <li>
            <a href="/2023/04/27/%E4%BD%A0%E5%A5%BD%EF%BC%8C%E6%88%91%E6%98%AFycy/">你好，我是ycy</a>
          </li>
        
          <li>
            <a href="/2023/04/27/hello/">hello</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>